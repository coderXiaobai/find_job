3.1 残差网络
    研究表明，网络的深度和宽度是表征网络复杂性的两个核心因素，且深度相比宽度更加有效。然而随着深度的增加，网络的训练愈发困难，这主要
是因为基于梯度下降的网络训练过程，误差信号的多层反向传播非常容易引起“梯度爆炸”(梯度过大导致出现NaN)或“梯度弥散”(梯度过小，反向传播
的误差极其微弱)。目前，一些特殊的权重初始化策略，以及批处理正规化(batch normalization)策略是这个问题得到了极大的改善，网络能够正常
训练了。但是，情况不容乐观，随着网络深度的增加，模型的训练误差不减反增了。
    这就出现了残差网络。首先介绍高速公路网络（highway network）：
    忽略网络层数，我们假设y = F(w(f),x),即当前层的输入为x，输出为y，w(f)表示w是和F对应的，F是非线性激活函数。那么高速网络将y定义为
            y = F(w(f),x) * T(w(t),x) + x * C(w(c),x)
    其中，T称为变换门，C称为携带门，分别负责控制变换的强度，以及对原始输入x的保留程度。其实y就是F和x的加权线性组合。一般将C设置为
1 - T，那么上式变为：
            y = F(w(f),x) * T(w(t),x) + x * (1 - T(w(t),x))
    这就保留了恢复原始输入的可能，当T为0时，y = x，即对输入x不变换;当T = 1时，y = F(w(f),x),即为常规网络
    深度残差网络：
    当高速网络模型中，T和C都为恒等映射时，那么
        y = F(w,x) + x
    简单变换：
        F(w,x) = y - x
    也就是说网络需要学习的参数w的非线性映射函数F，就是y - x,即残差，称为F为残差函数。
                                |
                              x |-------------
                                |             |
                           weight layer       |
                                |             |
                     F          |             |  x
                                |             |
                           weight layer       |
                                |             |
                                |             |
                                |             |
                              F + x ----------|
                                |
                                |
                                |
                              Relu
                                |
                                |
    整个残差模块如上图，F是学得的残差函数，然后和原始x进行简单的“+”操作，得到F + x再经过Relu即得到残差模块的输出。
                                |                                   |
                         x(64-d)|-------------           x(256 - d) |-------------
                                |             |                     |             |
                           3 * 3, 64          |                   1 * 1 ,64       |
                                | Relu        |                     | Relu        |
                     F          |             |  x                  |             |
                                |             |                   3 * 3, 64       |
                           3 * 3, 64          |                     | Relu        |
                                |             |                     |             |
                                |             |                  1 * 1, 256       |
                                |             |                     |             |
                              F + x ----------|                   F + x ----------|
                                |                                   |
                                |                                   |
                                |                                   |
                              Relu                                Relu
                                |                                   |
                           常规残差模块                         "瓶颈"残差模块
    左图为刚才提到的常规残差模块，由两个3×3卷积堆叠而成，但是随着网络深度的进一步增加，这种残差函数 在实践中并不是十分有效。右图所
示为“瓶颈残差模块”，依次由3个 1×1，3×3 和 1×1 的卷积层构成，这里 1×1 卷积能够起降维或者升维的作用，从而令3×3的卷积可以在相对较低
维度的输入上进行，以达到提高计算效率的目的。在非常深的网络中，“瓶颈残差模块”可大量减少计算代价。
