1 基本结构
    1.1
        * 前馈运算：通过卷积、汇合、非线性激励函数映射等一系列操作的层层堆叠，将高层语义信息逐层从原始数据输入层中抽取出来，逐层抽
        象。最终，cnn的最后一层将其目标任务（分类、回归）形式化为损失函数。
        * 反馈运算：通过计算预测值与真实值之间的loss，通过bp算法将loss从损失函数层逐层向前反馈，更新每层参数。重复此反馈操作，直至
        网络模型收敛，以达到训练模型的目的。
        更通俗的讲，其实cnn的操作过程犹如搭积木，将每一层的操作w(i)搭在前一层的数据上，直至损失函数层z结束:
            x(1) -> w(1) -> x(2) -> w(2) -> ... x(L - 1) -> w(L - 1) -> x(L) -> w(L) -> z
        若y是输入x(1)对应的真实标记，那么损失函数可以定义为：z = L(x(L),y)。
            以回归任务为例，常用的L2损失函数z = 1 / 2 * ||x(L) - y|| ^ 2
            以分类任务为例，常用的交叉熵损失函数 z = -sum(y(i) * log(p(i),2)), pi = exp(x(i,L) / sum(exp(x(j,L)))), 其中
            i = 1...C,C为分类数,j = 1...d,d为x(L)的维度(即分类数)
    1.2
        cnn模型依赖于最小化损失函数来学习模型的参数。常采用SGD和error bp策略来进行模型参数更新。具体来讲，主要使用mini-batch SGD
        * mini-batch SGD:
            首先，从数据集中随机采用n个样本，以这n个样本进行前馈运算，计算loss并使用梯度下降法更新参数，梯度从后往前逐层反馈，直至
            更新到网络的第一个参数，这样的一次处理过程称为一次批处理(mini-batch)过程。不同批处理过程按照不放回的采样遍历完所有数据，
            称为遍历了一轮(epoch)数据。
        * mini-batch size:
            不宜设置过小，因为每一个mini-batch的样本采样过程随机，因此根据这些数据的误差更新出来的模型参数不一定是全局最优更新(此时
            为局部最优更新)，会使得训练过程产生振荡。设置上限主要取决于硬件，一般设置为32，64，128，256
        * error bp:
            假设某一mini-batch处理前馈得到的误差为z，采用L2损失函数：
            容易得到，z对w(L)的偏导为0，z对x(L)的偏导为x(L) - y
            不难发现，其实每一层的操作对应了两部分导数：
                1 损失z对第i层参数w(i)的偏导a(i),用于更新该层的参数w(i)，w(i) = w(i) - yita * a(i),其中yita为SGD的步长，一般
                随着epoch的增大而减小。
                2 损失z对第i层输入x(i)的偏导b(i)，用于误差loss向前一层的反向传播，可将其视作误差从最后一层传播到第i层的误差信号
            下面以第i层的参数更新为例：
                当误差更新信号反向传播到第i层时，已知a(i + 1)和b(i + 1)，需要计算a(i)和b(i),根据链式法则:
                a(i) = z 对 x(i + 1) 的偏导 * x(i + 1) 对 w(i) 的偏导 = b(i + 1) * x(i + 1) 对 w(i) 的偏导
                    而 x(i + 1) = x(i) * w(i),所以:
                a(i) = b(i + 1) * x(i),于是：
                w(i) = w(i) - yita * a(i) = w(i) - yita * b(i + 1) * x(i)

                b(i) = z 对 x(i + 1) 的偏导 * x(i + 1) 对 x(i) 的偏导 = b(i + 1) * x(i + 1) 对 x(i) 的偏导
                    而 x(i + 1) = x(i) * w(i),所以:
                b(i) = b(i + 1) * w(i)
            更新完w(i)之后，再将此层的误差b(i)向前即第i - 1层继续传播，重复此过程，直至传播至第1层，那么通过采用一个mini-batch SGD
            策略来更新模型参数的过程就完成了。
            * 反向传播算法：
                输入：数据集data = (x(i),y(i)),i = 1...N,N为数据集数目，训练轮数(epoch):T,批处理大小:mini_batch_size
                输出：w(i),i = 1...L
                for t = 1...T:
                    while data:
                        从data中随机抽样mini_batch_size大小的数据，得到每批次的数据：batch_data
                        data -= batch_data
                        for i = L...1:
                            前馈计算得到误差z
                            if i == L:
                                计算z对w(L)的偏导，得到a(L)
                                计算z对x(L)的偏导，得到b(L)
                            else:
                                a(i) = b(i + 1) * x(i)
                                b(i) = b(i + 1) * w(i)
                                w(i) -= yita * b(i + 1) * x(i)
                return w

