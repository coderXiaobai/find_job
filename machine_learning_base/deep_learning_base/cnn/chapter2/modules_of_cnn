2.2 卷积层
    2.2.1 什么是卷积
        nn中的卷积操作仅涉及离散卷积情形。卷积操作就是通过一个卷积核(filter,一般为f * f大小，即正方形)，以一定的步长(stride)，
    填充(padding)，与输入数据在某一位置的部分(卷积核大小一样的部分)进行分别相乘求和，即得到新的输出数据在该位置的卷积值，这属于二维
    情况，更进一步的对于图像来说，有通道(channel)，是三维的，那么在channel上分别进行二维卷积操作，对这些卷积再求和，就得到了该位置
    上的最终卷积结果，但还是二维的，此时只需分别与d种不同的filter做卷积，在该位置便得到了d层的二维1 * 1结果了，这就对应了输出的
    channel大小。
        卷积还带来了权值共享的特性。假设简单情形，一个二维 3 * 3 的输入数据，如果用一个 2 * 2 的卷积核f(stride = 1,padding = 0)
    来进行卷积，你会发现输入的第4个元素会分别参与两次卷积操作，得到输出的第1，3个元素，而第5个元素则会参与4次卷积操作，得到输出的第
    1，2，3，4个元素。对比传统的全连接网络，每一个输入元素都会参与到每一个输出元素的计算中，且对应到每一个输出元素的权重都是不同的，
    而卷积能够共享权值得到输出。

    2.2.2 卷积的作用
        可以看出卷积操作是局部的，通过一定大小的卷积核作用于图像的局部区域获得局部信息。根据fiter中0处在不同的位置，分别有多种滤波器
    在进行卷积操作之后，由于0的存在，一些位置上的像素值为0，那么其余位置的信息便凸显出来，就可以学得图像的横向、纵向、边缘等信息。
        而事实上，cnn中的filter就是我们要优化的参数，是网络根据特定任务需求学习得来的，适用的filter。它可以检测出颜色、形状、纹理等
    多种信息，通过组合这些filter以及随着网络后续操作的进行，基本而一般的模式会被逐渐抽象为具有高层语义信息的“概念”表示，并以此对应
    到具体的样本类别。

2.3 汇合层
        汇合层一般包括max-pooling以及average-pooling，与普通卷积层不同的是，汇合层不需要学习参数，只需指定卷积参数(max/average,
    stride,padding,size)即可。

    2.3.1 什么是汇合
        意思很简单，就是对指定卷积区域做取得最大值或取得均值，不需要分别计算乘积再求和。

    2.3.2 汇合的作用
        其实汇合是一种“降采样”的过程，另外，汇合也可以看作是p-范数作为非线性映射的“卷积”操作，当p趋于无穷时即为常见的max-pooling。
        主要作用为：
        * 特征不变性：汇合操作使模型更加关注于是否存在某种特征而不是特征的位置。可看作是一种很强的先验，使特征学习包含某种程度自由
        度，能容忍一些特征微小的位移。
        * 特征降维。由于汇合操作的降采样作用，汇合结果中的一个元素对应于原输入数据的一个子区域，因此汇合相当于在空间范围内做了维度
        约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。
        * 在一定程度防止过拟合，更方便优化

2.4 激活函数
        激活函数层又称非线性映射层，顾名思义，激活函数的引入为的是增加整个网络的表达能力（即非线性）。否则，若干线性操作层的堆叠仍
    然只能起到线性映射的作用，无法形成复杂的函数。