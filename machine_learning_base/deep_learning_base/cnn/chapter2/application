1 数据扩充
    深度卷积网络自身拥有强大的表达能力，不过正因如此，网络本身需要大量甚 至海量数据来驱动模型训练，否则便有极大可能陷入过拟合的窘境。
可实际中，并不是所有数据集或真实任务都能提供如image-net数据集一般的海量训练样本。因此在实践中数据扩充（data augmentation）便成为
深度模型训练的第 一步。
    有效的数据扩充不仅能扩充训练样本数量，还能增加训练样本的多样性，
    一方面可避免过拟合，
    另一方面又会带来模型性能的提升
    1.1 简单的数据扩充方式
        水平翻转，随机扣取，尺度变换和旋转，增加网络对物体尺度和方向上的鲁棒性。色彩抖动。
    1.2 Fancy PCA
        可以近似的捕获自然图像的一个重要特性，即物体特质与光照强度和颜色变化无关。
        对R,G,B像素值进行主成分分析PCA操作，得到对应的特征向量pi和特征值λi，然后根据特征向量和特征值可以计算一组随机值
    [p1,p2,p3][α1λ1,α2λ2,α3λ3]⊤，将其作为扰动加到原像素值中即可。其中，αi为取自以0为均值，标准差为0.1的高斯分布的随机值。在每经
    过一轮训练（一个epoch）后，αi将重新随机选取并重复上述操作对原像素值进行扰动.

2 数据预处理
    在实践中，对每个特征减去平均值来中心化数据是非常重要的，这种归一化处理方式被称作“中心式归一化”（mean normalization）。卷积神经
网络中的数据预处理通常是计算训练集图像像素均值，之后在处理训练集、验证集和测试集图像时需要分别减去该均值。减均值操作的原理是，我们默
认自然图像是一类平稳的数据分布（即数据每一个维度的统计都服从相同分布），此时，在每个样本上减去数据的统计平均值（逐样本计算）可以移除
共同部分，凸显个体差异。
    需要注意的是，在实际操作中应首先划分好训练集、验证集和测试集，而该均值仅针对划分后的训练集计算。不可直接在未划分的所有图像上计算
均值，如此会违背机器学习基本原理，即“模型训练过程中能且仅能从训练数据中获取信息”。

3 网络参数初始化
    我们知道神经网络模型一般依靠随机梯度下降法进行模型训练和参数更新，网络的最终性能与收敛得到的最优解直接相关，而收敛效果实际上又很
大程度取决于网络参数最开始的初始化。理想的网络参数初始化使模型训练事半功倍，相反，糟糕的初始化方案不仅会影响网络收敛甚至会导致“梯度
弥散”或“爆炸”致使训练失败。
    3.1 全零初始化
        错误做法，虽然经过合理的预处理和规范化，网络模型趋于收敛时参数正负各半，即期望为0，而全零初始化的出发点在于期望相同，但这会
    导致网络神经元输出相同，那么梯度更新相同，其实根本就无法训练。
    3.2 随机初始化
        我们仍然希望能初始化一个期望接近0的参数分布，一般采用均值为0方差为1的标准高斯分布或均匀分布可以获得较好的初始化。
        上述做法仍会带来一个问题，即网络输出数据分布的方差会随着输入神经元个数改变。
        Xavier:基于上述高斯分布，加上一个对方差的大小的规范化，即对参数"/sqrt(n)"。具有更快收敛性。
    3.3 直接使用预处理模型参数

4 激活函数
    激活函数，是深度卷积神经网络中不可或缺的关键模块。可以说，深度网络模型其强大的表示能力大部分便是由激活函数的非线性带来的。
    sigmoid和relu已经讲过了
    4.1 tanh(x)型函数
        tanh(x)型函数是在sigmoid型函数基础上为解决均值问题提出的：
            tanh(x) = 2t(2x) - 1
        值域(-1,1),均值0，仍会出现和sigmoid一样的“梯度弥散”
    4.2 Leaky Relu
        在Relu的基础上，为了避免“死区”（即x < 0时，Relu的激活函数值直接为0），对x < 0的情况下添加了一个参数alpha，由于合适的值较难
    设定且敏感，所以在实际应用中并不常用。
    4.3 参数化、随机化Relu
        为了解决上述alpha较难设定的问题，提出了这两种。
        参数化：直接将alpha作为网络学习参数，融入整体训练中。但会增加过拟合风险。
        随机化：训练阶段使用均匀分布，测试阶段指定为训练阶段均匀分布的期望(l + u) / 2
            alpha ~ U(l,u),l < u,and l,u in [0,1)
        指数化：对x < 0的情况下(死区)，y = a * (exp(x) - 1)，不过增大了计算量，一般a设置为1
    4.4 总结
        首选Relu。
        想要继续提升精度，再在leaky、参数化、随机化、指数化之间选择尝试。优劣暂时无定论。
5 目标函数

